{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "parent_path = str(Path(__name__).resolve().parents[2]) + '/data/processed/'\n",
    "\n",
    "model_parent_path = str(Path(__name__).resolve().parents[2]) + '/data/'\n",
    "test_data_directory = parent_path + 'test'\n",
    "test_data_name = \"test_dataset.csv\"\n",
    "pipe_directory = parent_path + 'pipe'\n",
    "pipe_name = \"pipe.joblib\"\n",
    "model_directory = model_parent_path + 'model'\n",
    "onnx_model_name = \"machine_rf_0.onnx\"\n",
    "sk_model_name = \"machine_rf_0.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = os.path.join(test_data_directory, test_data_name)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(labels=[\"machine_failure\"], axis=1)\n",
    "y_test = test_data[\"machine_failure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_directory, onnx_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_path = os.path.join(pipe_directory, pipe_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load(pipe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('float_input', 'output_label')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "input_name, label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDI 는 포함되지 않은 인덱스입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = pipe.transform(X_test).to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_onx = sess.run([label_name], {input_name: inputs})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_onx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sess.run([label_name], {input_name: inputs})[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sess = rt.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "label = {\n",
    "    0: \"normal\",\n",
    "    1: \"failure\"\n",
    "}\n",
    "\n",
    "labels = []\n",
    "predicted = []\n",
    "trans_durations = []\n",
    "infer_durations = []\n",
    "for i, row in X_test.iterrows():\n",
    "    x = (pd.DataFrame([row]))\n",
    "    trans_start = time.time()\n",
    "    x_trans = pipe.transform(x).to_numpy().astype(np.float32)\n",
    "    trans_end = time.time()\n",
    "    pred_onx = sess.run([label_name], {input_name: x_trans})[0]\n",
    "    infer_end = time.time()\n",
    "    \n",
    "    labels.append(str(label[pred_onx[0]]))\n",
    "    predicted.append(pred_onx)\n",
    "    trans_durations.append(trans_end-trans_start)\n",
    "    infer_durations.append(infer_end - trans_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530.3888504505157, 36608, 0.014488331797708582)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = sum(trans_durations) + sum(infer_durations)\n",
    "total_transform_time = sum(trans_durations)\n",
    "total_infer_time = sum(infer_durations)\n",
    "total_tested = len(predicted)\n",
    "average_duration_second = total_time / total_tested\n",
    "average_transform_second = total_transform_time / total_tested\n",
    "average_infer_second = total_infer_time / total_tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "batch_labels = []\n",
    "batch_predicted = []\n",
    "batch_transform_durations = []\n",
    "batch_infer_durations = []\n",
    "\n",
    "batch_step = 0\n",
    "for i in range(0, X_test.shape[0], batch_size):\n",
    "    x = X_test[i:i+batch_size]\n",
    "    \n",
    "    batch_transform_start = time.time()\n",
    "    x_trans = pipe.transform(x).to_numpy().astype(np.float32)\n",
    "    batch_transform_end = time.time()\n",
    "    pred_onx = sess.run([label_name], {input_name: x_trans})[0]\n",
    "    batch_infer_end = time.time()\n",
    "    \n",
    "    batch_labels.extend([str(label[p]) for p in pred_onx.tolist()])\n",
    "    batch_predicted.extend([p for p in pred_onx.tolist()])\n",
    "    batch_transform_durations.append(batch_transform_end-batch_transform_start)\n",
    "    batch_infer_durations.append(batch_infer_end - batch_transform_end)\n",
    "    batch_step += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26291346549987793, 1144, 0.00022981946284954363)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_total_time = sum(batch_transform_durations) + sum(batch_infer_durations)\n",
    "batch_transform_total_time = sum(batch_transform_durations)\n",
    "batch_infer_total_time = sum(batch_infer_durations)\n",
    "average_batch_duration_second = batch_total_time / batch_step\n",
    "average_batch_transform_second = batch_transform_total_time / batch_step\n",
    "average_batch_infer_second = batch_infer_total_time / batch_step\n",
    "total_time, batch_step, average_batch_duration_second, average_batch_transform_second, average_batch_infer_second\n",
    "# (16.847609758377075, 1144, 0.014726931606972967) only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    ")\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "model_type = \"classification\"\n",
    "if model_type == \"classification\":\n",
    "    metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
    "if model_type == \"regression\":\n",
    "    metrics = [\n",
    "        root_mean_squared_error,\n",
    "        mean_squared_error,\n",
    "        mean_absolute_error,\n",
    "        r2_score,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "if model_type == \"classification\":\n",
    "    results[sess.__class__.__name__] = {\n",
    "        metric.__name__: (\n",
    "            metric(y_test, batch_predicted)\n",
    "            if metric.__name__ == \"accuracy_score\"\n",
    "            else metric(y_test, batch_predicted, average='macro')\n",
    "        )\n",
    "        for metric in metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceSession': {'accuracy_score': 0.990302666083916,\n",
       "  'precision_score': 0.9933166673846827,\n",
       "  'recall_score': 0.718587056899602,\n",
       "  'f1_score': 0.8014127283691965}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data_directory: str, preprocess_pipeline: Pipeline) -> Dict:\n",
    "\n",
    "    classifier = Classifier()\n",
    "\n",
    "    directory_list = os.listdir(test_data_directory)\n",
    "\n",
    "    predictions = {}\n",
    "    predicted = []\n",
    "    labels = []\n",
    "    durations = []\n",
    "\n",
    "    for c in directory_list:\n",
    "        c_path = os.path.join(c_path, f)\n",
    "        start = time.time()\n",
    "        x = classifier.predict_label(data)\n",
    "        end = time.time()\n",
    "        duration = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
